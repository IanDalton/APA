{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import kds\n",
    "import shap\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "from feature_engine.imputation import AddMissingIndicator, CategoricalImputer\n",
    "from feature_engine.encoding import CountFrequencyEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer,LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scikitplot.metrics import  plot_confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "Se crean variables para unificar y entrenar el modelo a ver si es cancer o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_cancer\"] = df[\"diagnostic\"].apply(lambda x: 1 if x in [\"BCC\",\"MEL\",\"SCC\"] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la persona estuvo aca antes? Tuvo en esta parte del cuerpo antes? Que tuvo antes? -> me parece que va a overfitear con eso\n",
    "\n",
    "\n",
    "x = df.drop(columns=[\"is_cancer\", \"diagnostic\"]) \n",
    "y1 = df[\"is_cancer\"]\n",
    "y2 = df[\"diagnostic\"]\n",
    "\n",
    "class HadBeforeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, y):\n",
    "        self.patient_counts = x[\"patient_id\"].value_counts()\n",
    "        #x[\"had_here_before\"] = x[\"patient_id\"].apply(lambda x: self.patient_counts[x] -1  )\n",
    "        return self\n",
    "    def transform(self, x:pd.DataFrame):\n",
    "        new_counts = x[\"patient_id\"].value_counts()\n",
    "        if new_counts.equals( self.patient_counts):\n",
    "            new_counts = {}\n",
    "\n",
    "        x[\"had_here_before\"] = x[\"patient_id\"].apply(lambda x: self.patient_counts.get(x,0)+ new_counts.get(x,0) -1   )\n",
    "        return x\n",
    "    \n",
    "class HadThisPartBeforeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, x:pd.DataFrame, _):\n",
    "        self.patient_region_counts = x.groupby([\"patient_id\", \"region\"]).size().unstack(fill_value=0).T.to_dict('dict')\n",
    "        return self\n",
    "    def transform(self, x:pd.DataFrame):\n",
    "        new_counts = x.groupby([\"patient_id\", \"region\"]).size().unstack(fill_value=0).T.to_dict('dict')\n",
    "        if new_counts == self.patient_region_counts:\n",
    "            new_counts = {}\n",
    "        x[\"had_this_part_before\"] = x.apply(lambda row: 1 if  new_counts.get(row[\"patient_id\"],{}).get(row[\"region\"],0) + self.patient_region_counts.get(row[\"patient_id\"],{}).get(row[\"region\"],0)>=2 else 0, axis=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class NormalDistributionImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables=None):\n",
    "        if not isinstance(variables, list):\n",
    "            self.variables = [variables]\n",
    "        else:\n",
    "            self.variables = variables\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we will store the mean and std of the variables for prediction time\n",
    "        self.param_dict_ = {}\n",
    "        for var in self.variables:\n",
    "            self.param_dict_[var] = {'mean': X[var].mean(), 'std': X[var].std()}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #TODO: Agregar seed para random\n",
    "        X = X.copy()\n",
    "        for feature in self.variables:\n",
    "            mu, std = self.param_dict_[feature]['mean'], self.param_dict_[feature]['std']\n",
    "            X[feature] = X[feature].apply(lambda x: np.random.normal(loc=mu, scale=std) if pd.isnull(x) else x)\n",
    "        return X\n",
    "\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, classifier_class, new_column_name,\n",
    "                  target_column=False, eta=0.3,max_depth=6 ,\n",
    "                   scale_pos_weight=1,min_child_weight=1,gamma =0,\n",
    "                   subsample=1,colsample_bytree=1,n_estimators=100):\n",
    "        self.classifier_class = classifier_class\n",
    "        self.eta = eta\n",
    "        self.max_depth = max_depth\n",
    "        self.scale_pos_weight = scale_pos_weight\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.n_estimators = n_estimators\n",
    "        self.gamma = gamma\n",
    "        self.classifier_kwargs = {\"eta\":eta, \"max_depth\":max_depth,\"scale_pos_weight\":scale_pos_weight}\n",
    "        self.new_column_name = new_column_name\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.classifier = self.classifier_class(eta=self.eta, max_depth=self.max_depth,\n",
    "        scale_pos_weight=self.scale_pos_weight,min_child_weight = self.min_child_weight,\n",
    "        gamma = self.gamma,subsample=self.subsample,colsample_bytree=self.colsample_bytree,n_estimators=self.n_estimators )\n",
    "\n",
    "        if self.target_column:\n",
    "            y = y.apply(lambda x: 1 if x in [\"BCC\",\"MEL\",\"SCC\"] else 0)\n",
    "        else:\n",
    "            y = self.label_encoder.fit_transform(y)\n",
    "\n",
    "        # Ensure x and y have the same length\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(f'Length of x ({len(x)}) does not match length of y ({len(y)})')\n",
    "\n",
    "        self.classifier.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        if not hasattr(self, 'classifier'):\n",
    "            raise RuntimeError('You must call fit before calling transform')\n",
    "\n",
    "        x_copy = x.copy()\n",
    "        x_copy[self.new_column_name] = self.classifier.predict(x)\n",
    "        return x_copy\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.label_encoder.inverse_transform(self.classifier.predict(x))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "pipe = imbPipeline([\n",
    "    ( \"had_here_before\", HadBeforeTransformer()),\n",
    "    ( \"had_this_part_before\", HadThisPartBeforeTransformer()),\n",
    "    (\"dropear\", FunctionTransformer(lambda x: x.drop([\"img_id\",\"lesion_id\", \"patient_id\"], axis=1,))),\n",
    "    (\"frequency\", CountFrequencyEncoder(encoding_method='frequency',variables=[\"region\"])),\n",
    "    (\"missingIndicator\",  AddMissingIndicator(variables=[\"smoke\",\"drink\",\"background_father\",\n",
    "                                        \"background_mother\",\"pesticide\",\"gender\",\"skin_cancer_history\",\n",
    "                                            \"cancer_history\", \"has_piped_water\",\"has_sewage_system\",\n",
    "                                            \"itch\",\"grew\",\"hurt\",\"changed\",\"bleed\",\"elevation\",\"fitspatrick\",\"diameter_1\",\"diameter_2\",\"age\"])),\n",
    "    (\"fillNaNs\", CategoricalImputer(imputation_method=\"missing\", variables=[\"smoke\",\"drink\",\"background_father\",\n",
    "                                        \"background_mother\",\"pesticide\",\"gender\",\"skin_cancer_history\",\n",
    "                                            \"cancer_history\", \"has_piped_water\",\"has_sewage_system\",\n",
    "                                            \"itch\",\"grew\",\"hurt\",\"changed\",\"bleed\",\"elevation\"])),\n",
    "    (\"ordinal\", OrdinalEncoder(encoding_method=\"arbitrary\", variables=[\"smoke\",\"drink\",\"background_father\",\n",
    "                                        \"background_mother\",\"pesticide\",\"gender\",\"skin_cancer_history\",\n",
    "                                            \"cancer_history\", \"has_piped_water\",\"has_sewage_system\",\n",
    "                                            \"itch\",\"grew\",\"hurt\",\"changed\",\"bleed\",\"elevation\"])),\n",
    "    (\"imputar_numericas\",NormalDistributionImputer(  variables= [\"fitspatrick\",\"diameter_1\",\"diameter_2\",\"age\"])),\n",
    "    (\"clf\",ClassifierTransformer(XGBClassifier, \"is_cancer\", target_column=True, scale_pos_weight=float(np.sum(y1 == 0))*100 / np.sum(y1 == 1))),\n",
    "    (\"smote\",SMOTE()),\n",
    "    (\"clf2\",ClassifierTransformer(XGBClassifier, \"diagnostic\",target_column=False))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x1_test,y1,y1_test = train_test_split(x,y2, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**{'clf__eta': 0.01,\n",
    "    'clf__max_depth': 10,\n",
    "    'clf__min_child_weight': 2,\n",
    "    'clf__gamma': 0,\n",
    "    \"clf__subsample\": 1,\n",
    "    \"clf__colsample_bytree\": 1,\n",
    "    \"clf__n_estimators\": 361,\n",
    "\n",
    "\n",
    "    'clf2__eta': 0.3,\n",
    "    'clf2__max_depth': 13,\n",
    "    'clf2__min_child_weight': 0,\n",
    "    'clf2__gamma': 0,\n",
    "    \"clf2__subsample\":1 ,\n",
    "    \"clf2__colsample_bytree\": 1,\n",
    "    \"clf2__n_estimators\": 1000,})\n",
    "pipe.fit(x1,y1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'clf__eta': (0.01, 0.3, 'uniform'),\n",
    "    'clf__max_depth': (1, 10, 'uniform'),\n",
    "    'clf__min_child_weight': (0,2,\"uniform\"),\n",
    "    'clf__gamma': (0,2,\"uniform\"),\n",
    "    \"clf__subsample\": (0.5,1,\"uniform\"),\n",
    "    \"clf__colsample_bytree\": (0.5,1,\"uniform\"),\n",
    "    \"clf__n_estimators\": (10,1000,\"uniform\"),\n",
    "\n",
    "\n",
    "    'clf2__eta': (0.001, 0.5, 'uniform'),\n",
    "    'clf2__max_depth': (1, 18, 'uniform'),\n",
    "    'clf2__min_child_weight': (0,2,\"uniform\"),\n",
    "    'clf2__gamma': (0,10,\"uniform\"),\n",
    "    \"clf2__subsample\": (0.5,1,\"uniform\"),\n",
    "    \"clf2__colsample_bytree\": (0.5,1,\"uniform\"),\n",
    "    \"clf2__n_estimators\": (10,5000,\"uniform\"),\n",
    "\n",
    "}\n",
    "# Create a BayesSearchCV object\n",
    "opt = BayesSearchCV(\n",
    "    pipe,\n",
    "    param_space,\n",
    "    n_iter=64,\n",
    "    cv=5, \n",
    "    n_jobs=-1,  # use all available cores\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Fit the BayesSearchCV object to the data\n",
    "opt.fit(x1,y1)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found: \", opt.best_params_)\n",
    "print(\"Best score found: \", opt.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = pipe.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1_pred = pipe.predict(x1_test)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y1_test)\n",
    "accuracy_score(encoder.transform(y1_test), encoder.transform(y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "class_order = ['BCC','MEL','SCC','ACK','NEV','SEK']\n",
    "plot_confusion_matrix(y1_test, y1_pred, normalize=False,labels=class_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb  = pipe[-3].classifier\n",
    "\n",
    "X_test:pd.DataFrame = pipe[:-2].transform(x1_test).drop(columns=[\"is_cancer\"])\n",
    "X_test[\"biopsed\"] = X_test[\"biopsed\"].apply(lambda x: 1 if x else 0)\n",
    "X_train = pipe[:-2].transform(x1).drop(columns=[\"is_cancer\"])\n",
    "X_train[\"biopsed\"] = X_train[\"biopsed\"].apply(lambda x: 1 if x else 0)\n",
    "explainer = shap.TreeExplainer(xgb,data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(df.corr()[\"diagnostic\"].sort_values(ascending=False))\n",
    "\n",
    "\n",
    "shap_values = explainer(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ABS_SHAP(df_shap,df):\n",
    "\n",
    "    shap_v = pd.DataFrame(df_shap.copy())\n",
    "    shap_v.columns = df.columns\n",
    "    df_v = df.copy().reset_index().drop('index',axis=1)\n",
    "    df_v = df_v.astype('float')\n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    feature_list = df.columns\n",
    "    for i in feature_list:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'#ff0051','#0076f1')\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = False).head(10)\n",
    "    k2.reset_index(inplace=True,drop=True)\n",
    "    k2['SHAP_abs']=round(k2['SHAP_abs'],2)\n",
    "    colorlist = k2['Sign']\n",
    "    ax=sns.barplot(data=k2,y='Variable',x='SHAP_abs',palette=colorlist,)\n",
    "    ax.set_xlabel(\"Mean SHAP Value\")\n",
    "    ax.legend([Line2D([0], [0], color='#ff0051', lw=4),Line2D([0], [0], color='#0076f1', lw=4)], ['Positiva', 'Negativa'],title='Correlación')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABS_SHAP(shap_values.values,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba=xgb.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "# Get the index of the maximum probability\n",
    "max_prob_index = y_pred_proba.argmax()\n",
    "min_prob_index = y_pred_proba.argmin()\n",
    "\n",
    "# Get the corresponding row in X_test\n",
    "max_index_row = X_test.iloc[[max_prob_index]]\n",
    "min_index_row = X_test.iloc[[min_prob_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "shap_values_f = explainer(max_index_row)\n",
    "\n",
    "# Create a waterfall plot\n",
    "shap.plots.waterfall(shap_values_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_f_min = explainer(min_index_row)\n",
    "\n",
    "shap.plots.waterfall(shap_values_f_min[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "shap.plots.force(shap_values_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values_f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = encoder.transform(y1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds.metrics.plot_cumulative_gain(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds.metrics.plot_lift(y_test, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds.metrics.plot_lift_decile_wise(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds.metrics.plot_ks_statistic(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kds.metrics.report(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reshape y_pred_proba into a 2D array if it's currently a 1D array\n",
    "if y_pred_proba.ndim == 1:\n",
    "    y_pred_proba = np.column_stack([1 - y_pred_proba, y_pred_proba])\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "skplt.metrics.plot_precision_recall(y_test, y_pred_proba, classes_to_plot=1, plot_micro=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' auc_macro=roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class=\"ovr\")\\nauc_weighted=roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class=\"ovr\")\\n '"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = y1_pred\n",
    "y_test = y1_test\n",
    "\"\"\" auc_macro=roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "auc_weighted=roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class=\"ovr\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACK       0.86      0.83      0.85       133\n",
      "         BCC       0.83      0.91      0.87       177\n",
      "         MEL       0.77      0.77      0.77        13\n",
      "         NEV       0.77      0.72      0.74        50\n",
      "         SCC       0.70      0.52      0.60        44\n",
      "         SEK       0.61      0.63      0.62        43\n",
      "\n",
      "    accuracy                           0.80       460\n",
      "   macro avg       0.76      0.73      0.74       460\n",
      "weighted avg       0.80      0.80      0.80       460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes in y_true not equal to the number of columns in 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m auc_macro\u001b[38;5;241m=\u001b[39m\u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m auc_weighted\u001b[38;5;241m=\u001b[39mroc_auc_score(y_test, y_pred_proba, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ianda\\anaconda3\\envs\\itba_apa_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ianda\\anaconda3\\envs\\itba_apa_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:620\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    624\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32mc:\\Users\\ianda\\anaconda3\\envs\\itba_apa_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:737\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    735\u001b[0m     classes \u001b[38;5;241m=\u001b[39m _unique(y_true)\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes) \u001b[38;5;241m!=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    738\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes in y_true not equal to the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_score\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    740\u001b[0m         )\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes in y_true not equal to the number of columns in 'y_score'"
     ]
    }
   ],
   "source": [
    "auc_macro=roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "auc_weighted=roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Kappa Sklearn: 0.7256259643936152\n"
     ]
    }
   ],
   "source": [
    "#Método incluido en sklearn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Kappa Sklearn: {cohen_kappa_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itba_apa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
